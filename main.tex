\documentclass[xcolor=dvipsnames]{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{animate}
\usepackage{dsfont}
\usepackage{minted}
\usepackage{tikz}

\usemintedstyle{default}

\definecolor{dark_blue}{RGB}{0,25,75}
\setbeamercolor{frametitle}{fg=White,bg=dark_blue!80}


\newcommand{\footlineB}{
\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=7ex,dp=2ex,left]{title in head/foot}%
	\hspace{0.5cm} \vspace{-0.06cm} \includegraphics[width=.19\paperwidth]{images/edi-shield.pdf}
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=7ex,dp=2ex,left,rightskip=.3cm]{title in head/foot}%
     \vspace{0.17cm}\hspace{-0.07cm}
     \textsf{\fontsize{4pt}{0cm}\selectfont
     \insertauthor  \newline 
	 \insertshortinstitute \hfill \insertframenumber
     }
    \end{beamercolorbox}%
    }%
  \vskip0pt%
}
}


\newcommand{\titlepageWhite}{ % % This block is for the white background title page
\setbeamertemplate{footline}
  {\hbox{%
    \begin{beamercolorbox}[wd=\paperwidth,dp=2ex,left]{title in head/foot}%
      %\hspace{-0.1cm} \vspace{0.52cm} 
      
      \vspace{-1.45cm}
      \includegraphics[width=\paperwidth]{images/edi-shield.pdf}
    \end{beamercolorbox}%
      }%
    \vskip0pt%
  }
\defbeamertemplate*{title page}{customized}[1][]
  { \vspace{2cm}
  	\begin{beamercolorbox}[wd=0.9\paperwidth,dp=2ex,left]{author in head/foot}%
    %\usebeamerfont{title}{\LARGE\inserttitle\par}
    \usebeamerfont{title}\textcolor{dark_blue!90}{\LARGE\inserttitle}
    \par
    \bigskip
    \usebeamerfont{author}\textcolor{dark_blue!90}{\normalsize\insertauthor.}
    \par
    \smallskip
    \bigskip
    \usebeamerfont{date}\insertdate\par
    \end{beamercolorbox}
  }
}


\title[IISWC 2018 Presentation]{Characterising Across Stack Optimisations for Convolutional Neural Networks}
\author{J. Turner, J. Cano, V. Radu, E. J. Crowley, M. Oâ€™Boyle, A. Storkey}
\institute{University of Edinburgh}
\date{}

\begin{document}

{\titlepageWhite
\begin{frame}
  \titlepage
\end{frame}
}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\section{Introduction}

\begin{frame}{Deep Learning is Kinda Cool}

\begin{itemize}
  \item Insert fun fact about deep learning that not everyone will know
  \item We \textit{want} to be able to deploy DL on edge devices because ... but we \textit{can't} because ...
\end{itemize}

\vskip 1cm

\begin{block}{Examples}
Maybe a nice figure of some stuff.
\end{block}

\end{frame}


\begin{frame}{Here's an ImageNet graph, sorry}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/imagenet_results.pdf}
\end{figure}


\end{frame}


\section{Challenges for Deployment}

\subsection{Large Models}

\begin{frame}{Deployment Challenges}

\begin{itemize}
	\item Models are too big for edge devices 
    \item Bottleneck in data movement
    \item Accessing off chip DRAM is expensive in terms of time and energy
\end{itemize}

\end{frame}

\begin{frame}{Parameter Redundancy}



\begin{itemize}
	\item Lucky for us... we know that the vast majority of parameters are redundant 
    \item Many different techniques for compressing networks but the effects are nontrivial
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=10cm]{images/speedup.pdf}
\end{figure}



\end{frame}


\begin{frame}{Contributions}
    
    \begin{itemize}
        \item Introduction of the \textit{Deep Learning Inference Stack}
        \item Characterisation of various neural network acceleration methods on different hardware
        \item Open source minimal implementations of common neural networks for benchmarking on embedded devices\footnote{https://github.com/jack-willturner/characterising-neural-compression}
    \end{itemize}
    
\end{frame}


\section{Background}

{
\setbeamercolor{background canvas}{bg=dark_blue!80}
\begin{frame}
    \centering
    \textcolor{white}{\LARGE Background}
\end{frame}
}

\begin{frame}{Background: Neurons}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/neuron.pdf}
\end{figure}

\end{frame}


\begin{frame}{Background: Affine Transform Neurons}



\end{frame}

\begin{frame}{Background: Convolution Neurons}
\centering
\animategraphics[loop,controls,width=0.6\linewidth]{1}{images/arbitrary_padding_no_strides_0}{0}{3}
\end{frame}

\begin{frame}{Background: Layers}

\begin{figure}
    \includegraphics[width=3cm]{images/layer.pdf}
\end{figure}

\end{frame}


\begin{frame}{Background: Neural Networks}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/neural_network.pdf}
\end{figure}
\end{frame}



\begin{frame}{Background: Weight Pruning}

\begin{itemize}
    \item Pruning at the individual \textit{weight} level
    \item Leaves weight matrices very sparse
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{images/weight_pruning.pdf}
\end{figure}

\end{frame}


\begin{frame}{Background: Channel Pruning}

\begin{itemize}
    \item Pruning at the \textit{neuron} level
    \item Leaves weight matrices small and dense
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{images/channel_pruning.pdf}
    \label{fig:channel_pruning}
\end{figure}

\end{frame}


\begin{frame}{Background: Quantisation}
\begin{itemize}
    \item Two options:
    \begin{enumerate}
        \item Reduce precision of weights
        \item Group to small set of centroids
    \end{enumerate}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=5cm]{images/quantization.pdf}
\end{figure}


\end{frame}


\section{Inference Stack}

{
\setbeamercolor{background canvas}{bg=dark_blue!80}
\begin{frame}
    \centering
    \textcolor{white}{\LARGE Experimental methodology}
\end{frame}
}

\begin{frame}{The Deep Learning Inference Stack}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/inference-stack.pdf}
    \caption{The Deep Learning Inference Stack}
    \label{fig:inference-stack}
\end{figure}
\end{frame}


\begin{frame}{Layer 1: Neural Networks}

\begin{columns}
\column{0.3\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/models.pdf}
    \label{fig:inference-stack-models}
\end{figure}

\column{0.7\textwidth}

\textbf{Neural Network Models}

\begin{itemize}
    \item Number of layers
    \item Number of neurons in each layer
    \item Operation each neuron performs
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=5.5cm]{images/neural_network.pdf}
\end{figure}

\end{columns}


\end{frame}

\definecolor{bg}{rgb}{0.95,0.95,0.95}

\begin{frame}{Layer 1: Neural Networks}
\vspace{0.2cm}

\inputminted[bgcolor=bg, fontfamily=cmss]{python}{images/mini_model.py}

\begin{figure}
    \centering
    \vspace{-1cm}
    \includegraphics[width=10cm]{images/lgoos.pdf}
\end{figure}
\end{frame}


\begin{frame}{Layer 1: Neural Networks}
\vspace{0.2cm}

\begin{itemize}
    \item VGG-16
    \item MobileNet
    \item ResNet-18
\end{itemize}

\end{frame}


\begin{frame}{Layer 2: Compression Techniques}

\begin{columns}
\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/compressions.pdf}
    \label{fig:inference-stack-compress-1}
\end{figure}

\column{0.7\textwidth}

\begin{enumerate}
    \item Weight Pruning
    \item Channel Pruning
    \item Quantisation
\end{enumerate}


\end{columns}

\end{frame}



\begin{frame}{Layer 2: Weight Pruning}

{\large\textbf{General Idea}}

\begin{itemize}
    \item Magnitude based thresholding 
    \item Iterate over steps of sparsifying and retraining 
    \item Leave the matrices very sparse but get up to 90\% compression rate
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/prune_retrain.pdf}
\end{figure}

{\footnotesize [Han, S., Mao, H. and Dally, W.J., 2015. Deep Compression]}
    
\end{frame}


\begin{frame}{Layer 2: Channel Pruning}
    
{\large\textbf{General Idea}}
\begin{itemize}
    \item Expand the error function into a Taylor series
    \item Take the second derivative wrt. error (i.e. how much removing neuron $n$ will affect error $e$)
    \item Weight with a FLOP penalty $\beta$ to bias towards removing expensive neurons
    \item Iteratively remove neurons and retrain
\end{itemize}

If we are optimising the parameters $\theta$ for some error function $\mathcal{L}$, then removing parameter $\theta_{k}$ will change the error by:
\begin{equation}
    \mathcal{L}(\theta - \theta_{ke_{k}}) - \mathcal{L}(\theta) = -g_{k}\theta_{k} + \frac{1}{2}H_{kk}\theta^2_{k}
\end{equation}

{\footnotesize [Theis et. al, 2018. Faster gaze prediction with dense networks and Fisher pruning.]}
    
\end{frame}


\begin{frame}{Layer 2: Quantisation}


{\large\textbf{General Idea}}
\begin{itemize}
    \item Set three centroid values and group all of the weights to each centroid
    \item Inference can be done with just these three values
    \item Often also have some sparsity (since one of the three values is usually zero)
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/quantization_pipeline.pdf}
\end{figure}
    
{\footnotesize [Zhu et. al, 2017. Trained Ternary Quantization.]}    

\end{frame}


\begin{frame}{Layer 3: Numerical Algorithms}

\begin{columns}

\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/numericals.pdf}
    \label{fig:inference-stack-nums-1}
\end{figure}

\column{0.7\textwidth}

{\large Direct Convolution}

\begin{figure}
    \centering
    \includegraphics[width=5cm]{images/arbitrary_padding_no_strides_01.pdf}
\end{figure}

\end{columns}
\end{frame}

\begin{frame}{Layer 3: Numerical Algorithms}

\begin{columns}

\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/numericals.pdf}
    \label{fig:inference-stack-nums-2}
\end{figure}

\column{0.7\textwidth}

{\large Convolution as SGEMM}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{images/im2col.pdf}
\end{figure}

\end{columns}
\end{frame}


\begin{frame}{Layer 3: Numerical Algorithms}

\begin{columns}

\column{0.3\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/numericals.pdf}
    \label{fig:inference-stack-nums-3}
\end{figure}

\column{0.7\textwidth}
\begin{itemize}
    \item CSR
\end{itemize}

\end{columns}
\end{frame}

\begin{frame}{Layer 4: Parallelisation Techniques}

\begin{columns}


\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/parallelisations.pdf}
    \label{fig:inference-stack-parallels}
\end{figure}

\column{0.7\textwidth}

\textbf{OpenMP}
\begin{itemize}
    \item CPU parallelisation of direct convolution 
    \item Something detailed about how OpenMP handles parallelisation
\end{itemize}

\end{columns}

\end{frame}


\begin{frame}{Layer 4: Parallelisation Techniques}

\begin{columns}


\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/parallelisations.pdf}
    \label{fig:inference-stack-parallels-2}
\end{figure}

\column{0.7\textwidth}

\textbf{OpenCL}
\begin{itemize}
    \item Tile size
    \item Register blocking 
    \item CLBlast Autotuning 
\end{itemize}

\end{columns}

\end{frame}



\begin{frame}{Layer 5: Hardware}

\begin{columns}


\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/hardwares.pdf}
\end{figure}

\column{0.7\textwidth}

\begin{itemize}
    \item CPU
    \item GPU
    \item Typical embedded device setup
\end{itemize}
\end{columns}


\end{frame}




\section{Experiments}

{
\setbeamercolor{background canvas}{bg=dark_blue!80}
\begin{frame}
    \centering
    \textcolor{white}{\LARGE Experiments}
\end{frame}
}

\begin{frame}{Experiments}

First experiment: compression vs. accuracy.
\vspace{0.5cm}
\begin{itemize}
    \item Quantisation performs particularly badly on MobileNet
    \item Weight pruning and channel pruning are roughly equivalent 
\end{itemize}
    
\vspace{0.5cm}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/accuracies.pdf}
\end{figure}
    
\end{frame}









%% in case you forget, the plotting code with better colours is kept in scratchpad
%% you need to update this before the presentation
{ % all template changes are local to this group.
    \setbeamertemplate{navigation symbols}{}
    \begin{frame}[plain]
        \begin{tikzpicture}[remember picture,overlay]
            \node[at=(current page.center)] {
                \includegraphics[width=\paperwidth]{images/plots.pdf}
            };
        \end{tikzpicture}
     \end{frame}
}











\begin{frame}{Experiments}
    
Show threading effect is the same on both devices 
    
\end{frame}







\begin{frame}{Experiments}

Show threading MobileNet slows it down.
    
\end{frame}



\begin{frame}{Experiments}
    
    \begin{figure}
        \centering
        \includegraphics[width=11cm]{images/mobilenet-odroid.pdf}
    \end{figure}
    
\end{frame}


\begin{frame}{Experiments}
    
    Now fix the accuracy at 90\% to make results more comparable.
    
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{images/accuracy.pdf}
    \end{figure}
    
\end{frame}




\begin{frame}{Experiments}
    
    Show speed and memory requirements.
    
\end{frame}



\begin{frame}{Experiments}
    
    Show GPU results.

    
\end{frame}








\begin{frame}{Conclusion}
    
    \begin{itemize}
        \item Channel pruning is the best, always
    \end{itemize}
    
\end{frame}


\begin{frame}{Further work}
\begin{itemize}
    \item More comprehensive benchmarking 
    \item Sparsity for robust networks
    \item Different sparse formats 
    \item Tuning parameters of direct convolution and im2col
\end{itemize}
\end{frame}

\begin{frame}{Thanks}
    \begin{itemize}
        \item Bonseyes
        \item EPSRC (PPar)
    \end{itemize}
\end{frame}




\end{document}