\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{animate}
\usepackage{minted}

\usemintedstyle{default}

\title[IISWC 2018 Presentation]{Characterising Across Stack Optimisations for Deep Convolutional Neural Networks}
\author{J. Turner, J. Cano, V. Radu, E. J. Crowley, M. Oâ€™Boyle, A. Storkey}
\institute{University of Edinburgh}
\date{}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\section{Introduction}

\begin{frame}{Deep Learning is Kinda Cool}

\begin{itemize}
  \item Insert fun fact about deep learning that not everyone will know
  \item Don't want people to tune out already
  \item We \textit{want} to be able to deploy DL on edge devices because ... but we \textit{can't} because ...
\end{itemize}

\vskip 1cm

\begin{block}{Examples}
Maybe a nice figure of some stuff.
\end{block}

\end{frame}

\section{Challenges for Deployment}

\subsection{Large Models}

\begin{frame}{Deployment Challenges}

\begin{itemize}
	\item Models are too big for edge devices 
    \item Bottleneck in data movement
    \item Accessing off chip DRAM is expensive in terms of time and energy
\end{itemize}

\end{frame}

\begin{frame}{Parameter Redundancy}

\begin{itemize}
	\item Lucky for us... we know that the vast majority of parameters are redundant 
    \item Many different techniques for compressing networks but the effects are nontrivial
    \item Show graph of expected speedup vs observed speedup
\end{itemize}

\end{frame}


\begin{frame}{Contributions}
    
    \begin{itemize}
        \item Introduction of the \textit{Deep Learning Inference Stack}
        \item Characterisation of various neural network acceleration methods on different hardware
    \end{itemize}
    
\end{frame}


\section{Background}

\begin{frame}{Background: Neurons}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/neuron.pdf}
\end{figure}

\end{frame}


\begin{frame}{Background: Affine Transform Neurons}



\end{frame}

\begin{frame}{Background: Convolution Neurons}
\centering
\animategraphics[loop,controls,width=0.6\linewidth]{1}{images/arbitrary_padding_no_strides_0}{0}{3}
\end{frame}

\begin{frame}{Background: Layers}

\begin{figure}
    \includegraphics[width=3cm]{images/layer.pdf}
\end{figure}

\end{frame}


\begin{frame}{Background: Neural Networks}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/neural_network.pdf}
\end{figure}
\end{frame}



\begin{frame}{Background: Weight Pruning}

\begin{itemize}
    \item Pruning at the individual \textit{weight} level
    \item Leaves weight matrices very sparse
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{images/weight_pruning.pdf}
\end{figure}

\end{frame}


\begin{frame}{Background: Channel Pruning}

\begin{itemize}
    \item Pruning at the \textit{neuron} level
    \item Leaves weight matrices small and dense
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{images/channel_pruning.pdf}
    \label{fig:channel_pruning}
\end{figure}

\end{frame}


\begin{frame}{Background: Quantisation}
\begin{itemize}
    \item Two options:
    \begin{enumerate}
        \item Reduce precision of weights
        \item Group to small set of centroids
    \end{enumerate}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=5cm]{images/quantization.pdf}
\end{figure}


\end{frame}


\section{Inference Stack}

\begin{frame}{The Deep Learning Inference Stack}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/inference-stack.pdf}
    \caption{The Deep Learning Inference Stack}
    \label{fig:inference-stack}
\end{figure}
\end{frame}

\begin{frame}{Layer 1: Neural Networks}

\begin{columns}
\column{0.3\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/models.pdf}
    \label{fig:inference-stack-models}
\end{figure}

\column{0.7\textwidth}

\textbf{Neural Network Models}

\begin{itemize}
    \item Number of layers
    \item Number of neurons in each layer
    \item Operation each neuron performs
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=5.5cm]{images/neural_network.pdf}
\end{figure}

\end{columns}


\end{frame}

\definecolor{bg}{rgb}{0.95,0.95,0.95}

\begin{frame}{Layer 1: Neural Networks}
\vspace{0.2cm}

\inputminted[bgcolor=bg, fontfamily=cmss]{python}{images/mini_model.py}

\begin{figure}
    \centering
    \vspace{-1cm}
    \includegraphics[width=10cm]{images/lgoos.pdf}
\end{figure}
\end{frame}


\begin{frame}{Layer 1: Neural Networks}
\vspace{0.2cm}

\begin{itemize}
    \item VGG-16
    \item MobileNet
    \item ResNet-18
\end{itemize}

\end{frame}


\begin{frame}{Layer 2: Compression Techniques}

\begin{columns}
\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/compressions.pdf}
    \label{fig:inference-stack-compress-1}
\end{figure}

\column{0.7\textwidth}

\begin{enumerate}
    \item Weight Pruning
    \item Channel Pruning
    \item Quantisation
\end{enumerate}


\end{columns}

\end{frame}



\begin{frame}{Layer 2: Weight Pruning}

\begin{itemize}
    \item Deep compression
    \item Magnitude based thresholding 
    \item Iterate over steps of sparsifying and retraining 
    \item Leave the matrices very sparse but get up to 90\% compression rate
\end{itemize}
    
\end{frame}


\begin{frame}{Layer 2: Channel Pruning}
    
\begin{itemize}
    \item Fisher pruning
    \item Taylor expansion of the objective
    \item Weighted with a FLOP penalty $\beta$
\end{itemize}
    
\end{frame}


\begin{frame}{Layer 2: Quantisation}
\begin{itemize}
    \item TTQ
    \item Similar thresholding concept to weight pruning
    \item Group to a set of three values 
    \item Inference can be done with just these three values
    \item Often also have some sparsity (since one of the three values is zero)
\end{itemize}
    
\end{frame}


\begin{frame}{Layer 3: Numerical Algorithms}

\begin{columns}

\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/numericals.pdf}
    \label{fig:inference-stack-nums-1}
\end{figure}

\column{0.7\textwidth}

\begin{itemize}
    \item Direct convolution
    \item \texttt{im2col}
    \item FFT and Winograd
\end{itemize}

\end{columns}
\end{frame}


\begin{frame}{Layer 3: Numerical Algorithms}

\begin{columns}

\column{0.3\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/numericals.pdf}
    \label{fig:inference-stack-nums-2}
\end{figure}

\column{0.7\textwidth}
\begin{itemize}
    \item CSR
\end{itemize}

\end{columns}
\end{frame}

\begin{frame}{Layer 4: Parallelisation Techniques}

\begin{columns}


\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/parallelisations.pdf}
    \label{fig:inference-stack-parallels}
\end{figure}

\column{0.7\textwidth}

\textbf{OpenMP}
\begin{itemize}
    \item CPU parallelisation of direct convolution 
    \item Something detailed about how OpenMP handles parallelisation
\end{itemize}

\end{columns}

\end{frame}


\begin{frame}{Layer 4: Parallelisation Techniques}

\begin{columns}


\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/parallelisations.pdf}
    \label{fig:inference-stack-parallels-2}
\end{figure}

\column{0.7\textwidth}

\textbf{OpenCL}
\begin{itemize}
    \item Tile size
    \item Register blocking 
    \item CLBlast Autotuning 
\end{itemize}

\end{columns}

\end{frame}



\begin{frame}{Layer 5: Hardware}

\begin{columns}


\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/hardwares.pdf}
\end{figure}

\column{0.7\textwidth}

\begin{itemize}
    \item CPU
    \item GPU
    \item Typical embedded device setup
\end{itemize}
\end{columns}


\end{frame}




\section{Experiments}

\begin{frame}{Experiments}

First experiment: take all of the models and apply the compression techniques to them to see their impact on accuracy.
This is good context for the rest of the analysis.
    
\end{frame}











\begin{frame}{Experiments}

Show the pareto elbows and their impact on inference time
    
\end{frame}











\begin{frame}{Experiments}
    
Show threading effect is the same on both devices 
    
\end{frame}







\begin{frame}{Experiments}

Show threading MobileNet slows it down.
    
\end{frame}



\begin{frame}{Experiments}
    
    Show layer by layer graph
    
\end{frame}


\begin{frame}{Experiments}
    
    Now fix the accuracy at 90\% to make results more comparable.
    Show the slice graph.
    
\end{frame}




\begin{frame}{Experiments}
    
    Show speed and memory requirements.
    
\end{frame}



\begin{frame}{Experiments}
    
    Show GPU results.

    
\end{frame}

\end{document}







\begin{frame}{Conclusion}
    
    \begin{itemize}
        \item Not always trivial
    \end{itemize}
    
\end{frame}