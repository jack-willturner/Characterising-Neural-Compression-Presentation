\documentclass[xcolor=dvipsnames]{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{animate}
\usepackage{dsfont}
\usepackage{minted}
\usepackage{tikz}

\usemintedstyle{default}

\definecolor{dark_blue}{RGB}{0,25,75}
\setbeamercolor{frametitle}{fg=White,bg=dark_blue!80}


\newcommand{\footlineB}{
\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=7ex,dp=2ex,left]{title in head/foot}%
	\hspace{0.5cm} \vspace{-0.06cm} \includegraphics[width=.19\paperwidth]{images/edi-shield.pdf}
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=7ex,dp=2ex,left,rightskip=.3cm]{title in head/foot}%
     \vspace{0.17cm}\hspace{-0.07cm}
     \textsf{\fontsize{4pt}{0cm}\selectfont
     \insertauthor  \newline 
	 \insertshortinstitute \hfill \insertframenumber
     }
    \end{beamercolorbox}%
    }%
  \vskip0pt%
}
}


\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

\newcommand{\titlepageWhite}{ % % This block is for the white background title page
\setbeamertemplate{footline}
  {\hbox{%
    \begin{beamercolorbox}[wd=\paperwidth,dp=2ex,left]{title in head/foot}%
      %\hspace{-0.1cm} \vspace{0.52cm} 
      
      \vspace{-1.45cm}
      \includegraphics[width=\paperwidth]{images/edi-shield.pdf}
    \end{beamercolorbox}%
      }%
    \vskip0pt%
  }
\defbeamertemplate*{title page}{customized}[1][]
  { \vspace{2cm}
  	\begin{beamercolorbox}[wd=0.9\paperwidth,dp=2ex,left]{author in head/foot}%
    %\usebeamerfont{title}{\LARGE\inserttitle\par}
    \usebeamerfont{title}\textcolor{dark_blue!90}{\LARGE\inserttitle}
    \par
    \bigskip
    \usebeamerfont{author}\textcolor{dark_blue!90}{\normalsize\insertauthor.}
    \par
    \smallskip
    \bigskip
    \usebeamerfont{date}\insertdate\par
    \end{beamercolorbox}
  }
}


\title[IISWC 2018 Presentation]{Characterising Across-Stack Optimisations for Convolutional Neural Networks}
\author{\underline{J. Turner}, J. Cano, V. Radu, E. J. Crowley, M. Oâ€™Boyle, A. Storkey}
\institute{University of Edinburgh}
\date{}

\begin{document}

{\titlepageWhite
\begin{frame}
  \titlepage
\end{frame}
}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\begin{frame}{Thanks}
    \begin{itemize}
        \item Bonseyes
        \item EPSRC (PPar)
    \end{itemize}
\end{frame}


\section{Introduction}

\begin{frame}{Neural Networks are large, and growing}

\begin{columns}

\column{0.5\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=5cm]{images/flop-growth.pdf}
    \label{fig:flop-growth}
    \caption{ImageNet winners}
\end{figure}

\column{0.5\textwidth}


\vspace{0.5cm}
\begin{itemize}
    \item High memory footprint
    \item Slow inference time 
    \item Large energy requirement
\end{itemize}

\end{columns}


\end{frame}


%\begin{frame}{Why Neural Networks?}

%\begin{figure}
%    \centering
%    \includegraphics[width=\linewidth]{images/imagenet_results.pdf}
%\end{figure}


%\end{frame}


\section{Challenges for Deployment}

\subsection{Large Models}


\begin{frame}{Speedup does not align with expectations}

\begin{itemize}
	\item In state-of-the-art networks, up to \textbf{90\%} of weights are redundant
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=10cm]{images/speedup.pdf}
\end{figure}



\end{frame}


\begin{frame}{Different hardware demands different compression}

\begin{figure}
    \centering
    \includegraphics{images/pareto_efficiency.pdf}
\end{figure}

[DPP-Net]
\end{frame}


\begin{frame}{The Deep Learning Inference Stack}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/inference-stack.pdf}
    \caption{The Deep Learning Inference Stack}
\end{figure}
\end{frame}



\section{Background}

{
\setbeamercolor{background canvas}{bg=dark_blue!80}
\begin{frame}
    \centering
    \textcolor{white}{\LARGE Background}
\end{frame}
}


%\begin{frame}{Background: Neurons}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.8\textwidth]{images/neuron.pdf}
%\end{figure}

%\end{frame}


%\begin{frame}{Background: Affine Transform Neurons}

%Multiply each input element $x$ by a weight $w$ and add a scalar bias term $b$:

%\begin{figure}
%    \centering
%    \includegraphics[width=0.8\linewidth]{images/affine_transform.pdf}
%\end{figure}

%\end{frame}

%\begin{frame}{Background: Layers}

%\begin{figure}
%    \includegraphics[width=3cm]{images/layer.pdf}
%\end{figure}

%\end{frame}


%\begin{frame}{Background: Neural Networks}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.6\linewidth]{images/neural_network.pdf}
%\end{figure}
%\end{frame}


%\begin{frame}{Background: Convolution Neurons}
%\centering
%\animategraphics[loop,controls,width=0.6\linewidth]{1}{images/arbitrary_padding_no_strides_0}{0}{3}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.6\linewidth]{images/arbitrary_padding_no_strides_01.pdf}
%    \label{fig:conv2d}
%\end{figure}

%\end{frame}


\begin{frame}{Background: Weight Pruning}

\begin{itemize}
    \item Pruning at the individual \textit{weight} level
    \item Leaves weight matrices very sparse
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{images/weight_pruning.pdf}
\end{figure}

\end{frame}


\begin{frame}{Background: Channel Pruning}

\begin{itemize}
    \item Pruning at the \textit{neuron} level
    \item Leaves weight matrices small and dense
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{images/channel_pruning.pdf}
    \label{fig:channel_pruning}
\end{figure}

\end{frame}


\begin{frame}{Background: Quantisation}
\begin{itemize}
    \item Two options:
    \begin{enumerate}
        \item Reduce precision of weights
        \item Group to small set of centroids
    \end{enumerate}
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=5cm]{images/quantization.pdf}
\end{figure}


\end{frame}

\section{Inference Stack}

{
\setbeamercolor{background canvas}{bg=dark_blue!80}
\begin{frame}
    \centering
    \textcolor{white}{\LARGE Experimental methodology}
\end{frame}
}



\begin{frame}{CIFAR-10}

Picture / description of CIFAR-10 will go here.

\end{frame}


\begin{frame}{Layer 1: Neural Networks}

\begin{columns}
\column{0.3\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/models.pdf}
    \label{fig:inference-stack-models}
\end{figure}

\column{0.7\textwidth}

\textbf{Neural Network Models}


\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
        Model & Accuracy & Weight size \\
        \hline
        VGG-16 & 92.0\% & 309.1 MB \\
        MobileNet & 91.5\% & \textbf{66.3 MB}\\
        ResNet-18 & \textbf{94.2\%} & 233.8 MB \\
    \end{tabular}
\end{table}

\end{columns}


\end{frame}

\definecolor{bg}{rgb}{0.95,0.95,0.95}

%\begin{frame}{Layer 1: Neural Networks}
%\vspace{0.2cm}

%\inputminted[bgcolor=bg, fontfamily=cmss]{python}{images/mini_model.py}

%\begin{figure}
%    \centering
%    \vspace{-1cm}
%    \includegraphics[width=10cm]{images/lgoos.pdf}
%\end{figure}
%\end{frame}


%\begin{frame}{Layer 1: Neural Networks}
%\vspace{0.2cm}

%\begin{figure}
%    \centering
%    \includegraphics[width=0.8\linewidth]{images/vgg.pdf}
%    \caption{VGG-16}
%\end{figure}


%\begin{figure}
%    \centering
%    \includegraphics[width=0.8\linewidth]{images/resnet.pdf}
%    \caption{ResNet-18}
%\end{figure}

%{\color{red} MobileNet figure goes here}

%\end{frame}


\begin{frame}{Layer 2: Compression Techniques}

\begin{columns}
\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/compressions.pdf}
    \label{fig:inference-stack-compress-1}
\end{figure}

\column{0.7\textwidth}
We consider representatives from three different classes of compression technique:
\begin{enumerate}
    \item Weight Pruning
    \item Channel Pruning
    \item Quantisation
\end{enumerate}


\end{columns}

\end{frame}



\begin{frame}{Layer 2: Weight Pruning}

{\large\textbf{General Idea}}

\begin{itemize}
    \item Magnitude based thresholding 
    \item Iterate over steps of sparsifying and retraining 
    \item Leave the matrices very sparse but get up to 90\% compression rate
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/prune_retrain.pdf}
\end{figure}

{\footnotesize [Han, S., Mao, H. and Dally, W.J., 2015. Deep Compression]}
    
\end{frame}


\begin{frame}{Layer 2: Channel Pruning}
    
{\large\textbf{General Idea}}
\begin{itemize}
    \item Use second-order Taylor expansion of error function to estimate effect of removing neuron \textit{n}
    \item Weight with a FLOP penalty $\beta$ to bias towards removing expensive neurons
    \item Iteratively remove neurons and retrain
\end{itemize}

%If we are optimising the parameters $\theta$ for some error function $\mathcal{L}$, then removing parameter $\theta_{k}$ will change the error by:
%\begin{equation}
%    \mathcal{L}(\theta - \theta_{ke_{k}}) - \mathcal{L}(\theta) = -g_{k}\theta_{k} + \frac{1}{2}H_{kk}\theta^2_{k}
%\end{equation}

\vspace{2cm}

{\footnotesize [Theis et. al, 2018. Faster gaze prediction with dense networks and Fisher pruning.]}
    
\end{frame}


\begin{frame}{Layer 2: Quantisation}


{\large\textbf{General Idea}}
\begin{itemize}
    \item Set three centroid values and group all of the weights to each centroid
    \item Inference can be done with just these three values
    \item Often also have some sparsity (since one of the three values is usually zero)
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/quantization_pipeline.pdf}
\end{figure}
    
{\footnotesize [Zhu et. al, 2017. Trained Ternary Quantization.]}    

\end{frame}


\begin{frame}{Layer 3: Numerical Algorithms}

\begin{columns}

\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/numericals.pdf}
    \label{fig:inference-stack-nums-1}
\end{figure}

\column{0.7\textwidth}

{\large Direct Convolution\footnotemark}

\begin{figure}
    \centering
    \includegraphics[width=5cm]{images/arbitrary_padding_no_strides_01.pdf}
\end{figure}

\end{columns}

\footnotetext{https://github.com/vdumoulin/conv\_arithmetic}

\end{frame}

\begin{frame}{Layer 3: Numerical Algorithms}

\begin{columns}

\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/numericals.pdf}
    \label{fig:inference-stack-nums-2}
\end{figure}

\column{0.7\textwidth}

{\large Convolution as SGEMM \footnotemark}

\begin{figure}
    \centering
    \includegraphics[width=8cm]{images/im2col.pdf}
\end{figure}

\end{columns}

\footnotetext{http://cs231n.stanford.edu/slides/2016/winter1516\_lecture11.pdf}
\end{frame}


\begin{frame}{Layer 3: Numerical Algorithms}

\begin{columns}

\column{0.3\textwidth}

\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/numericals.pdf}
    \label{fig:inference-stack-nums-3}
\end{figure}

\column{0.7\textwidth}
When the matrices are very sparse, we may wish to use a sparse representation format: 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/CSR.pdf}
\end{figure}

\end{columns}
\end{frame}

\begin{frame}{Layer 4: Parallelisation Techniques}

\begin{columns}


\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/parallelisations.pdf}
    \label{fig:inference-stack-parallels}
\end{figure}

\column{0.7\textwidth}

\textbf{OpenMP}
\begin{itemize}
    \item CPU parallelisation of direct convolution 
    \item Code snippet w. pragmas will go here
\end{itemize}

\end{columns}

\end{frame}


\begin{frame}{Layer 4: Parallelisation Techniques}

\begin{columns}


\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/parallelisations.pdf}
    \label{fig:inference-stack-parallels-2}
\end{figure}

\column{0.7\textwidth}

\textbf{OpenCL}
\begin{itemize}
    \item Hand tuned work groups and tiling
    \item CLBlast Autotuning 
\end{itemize}

\end{columns}

\end{frame}



\begin{frame}{Layer 5: Hardware}

\begin{columns}


\column{0.3\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=3.5cm]{images/hardwares.pdf}
\end{figure}

\column{0.7\textwidth}

\begin{itemize}
    \item Intel Core i7 (4 cores)
    \item Odroid-XU4 (ARM big.LITTLE CPU (8 cores) + Mali T628 GPU (6 cores)
\end{itemize}
\end{columns}


\end{frame}




\section{Experiments}

{
\setbeamercolor{background canvas}{bg=dark_blue!80}
\begin{frame}
    \centering
    \textcolor{white}{\LARGE Experiments}
\end{frame}
}

\begin{frame}{Impact of Compression on Accuracy}

\begin{itemize}
    \item Quantisation performs particularly badly on MobileNet
    \item Channel pruning $>$ weight pruning
\end{itemize}
    
\vspace{0.5cm}
\begin{figure}
    \centering
    \includegraphics[width=1.08\linewidth]{images/accuracies.pdf}
\end{figure}
    
\end{frame}



\begin{frame}{Impact of Compression on Speed}
    
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{images/vgg_speedup.pdf}
    \end{figure}
\end{frame}





\begin{frame}{Experiments}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/last_two.pdf}
\end{figure}
    
\end{frame}



\begin{frame}{Experiments}
    
    \begin{figure}
        \centering
        \includegraphics[width=10cm]{images/mobilenet-odroid.pdf}
    \end{figure}
    
\end{frame}


\begin{frame}{Experiments}
    
    Now fix the accuracy at 90\% to make results more comparable.
    
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{images/accuracy.pdf}
        \caption{Slicing the accuracy of each model at 90\%. (a) shows weight pruning, (b) shows channel pruning, (c) shows quantisation.}
    \end{figure}
    
\end{frame}




\begin{frame}{Experiments}
    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{images/inf-mem2.pdf}
        \caption{Inference time with the maximum number of threads on the Odroid and Intel when sliced at 90\% accuracy.}
        \label{fig:inf-mem}
    \end{figure}
\end{frame}

\begin{frame}{Parallelisation on ARM Mali GPU}
    

    \begin{figure}
        \centering
        \includegraphics[width=\linewidth]{images/Odroid-GPU.pdf}
        \label{fig:odroid-gpu}
    \end{figure}
    
\end{frame}



\begin{frame}{Conclusion}
    
    \begin{itemize}
        \item Channel pruning is the best, always
        \item FLOPs are \textbf{not} a perfect proxy for inference time
        \item Acceleration patterns are both network \textbf{and} hardware dependent, so you must account for both sides of the stack when deploying deep networks 
    \end{itemize}
    
\end{frame}






\end{document}